---
layout: post
title:  "movie"
date:   2020-05-03 08:25
categories: jekyll
permalink: /archivers/movie
---


파이썬 웹 크롤링 기초 (내이버 영화 댓글)

**목차** 

1.  **BeautifulSoup 시작하기**
2.  **태그(Tag) 탐색하기**
3.  **웹사이트 구조 분석하기 (개발자 도구)**
4.  **`Select`을 통해 원하는 부분 가져오기**
5.  **CSS 선택자를 통해 원하는 부분 가져오기**
6.  **텍스트만 읽어오기**

<!--more-->

## 들어가기에 앞서 ###
일단 웹 사이트라는 걸 이해해야 한다. 기본적으로 웹사이트는 문서다. HTML이라는 형식으로 쓰여진 문서.

그래서 우리는 HTML 문서에 담긴 내용을 가져 오도록  **request**(요청) 해야 한다. 파이썬에는 애초에  `requests`라는 라이브러리로 편리하게 사용이 가능하다. (만약 설치가 안 되어 있다면 pip를 통해 설치하고 사용하자.)

이제 아래와 같이  `requests.get()`안에 url을 넣어서 사용할 수 있다. 예를 들어  [내이버 영화 댓글]([https://movie.naver.com/movie/sdb/rank/rmovie.nhn](https://movie.naver.com/movie/sdb/rank/rmovie.nhn))을 가져오고 싶다면 이렇게.
~~~py
import requests

webpage = requests.get("https://movie.naver.com/movie/sdb/rank/rmovie.nhn")

print(webpage.text)
~~~
코드를 실행하면 당근마켓 홈 화면의 HTML 문서 전체를 긁어서 출력해준다.

물론 HTML 문서를 제대로 읽으려면 문법을 이해하고 있어야 하겠지만, 너무 깊이 들어가진 말자. 어차피 여기서 필요한 부분만 뽑아내는 방법을 익히면 되는데, 여기서부터가 본격적인 크롤링이다.

## 1. BeautifulSoup 시작하기 ###

요청한 HTML을 프린트 해보면 매우 길고 지저분하다. 그래서 결국 라이브러리의 힘을 빌려야 한다. HTML 문서를 탐색해서 원하는 부분만 쉽게 뽑아낼 수 있는 파이썬 라이브러리  **BeautifulSoup**를 사용해보자.

아래와 같이 코드를 추가해봤다.
~~~phthon
import requests  
from bs4 import BeautifulSoup  
  
webpage = requests.get("https://movie.naver.com/movie/sdb/rank/rmovie.nhn")  
  
soup = BeautifulSoup(webpage.content, "html.parser")  
  
print(soup)
~~~
일단  `from bs4 import BeautifulSoup`로 라이브러리를 불러올 수 있다.

그리고 웹페이지를 요청한 뒤, 여기서 받아낸 문서를  `.content`로 지정한 후  `BeautifulSoup`를 통해 soup라는 객체로 저장하면 된다.

여기서 뒤에  `"html.parser"`라고 덧붙였다. 이외에도  `"lxml"`, `"html5lib"`  등의 옵션을 사용할 수 있으며 각각의 장단점이 있는데, 어쨌든 자세한 내용은 생략하고 (사실 나도 잘 모르니까) 그냥 일단 html로 하자.


## 2. 태그(Tag) 탐색하기 ###

 HTML 문서를 잘 보면 < > 기호를 활용해서 다양한 태그를 사용한다. HTML 문법까지 설명하면 너무 깊어지니… 넘어가자.

어쨌든 p 태그만 찾아서 출력해보자. 이렇게 하면 첫번째 p 태그를 찾아 준다.
~~~phthon
print(soup.p)
~~~
---
~~~html
 <p class="r_date">2020.05.08 
	 <a href="rmovie.nhn?sel=cnt&amp;tg=0&amp;date=20200507">
		 <img alt="prev" height="13" src="https://ssl.pstatic.net/imgmovie/2007/img/common/btn_prev.gif" style="margin-right:1px;" width="13"/>
	 </a>
 </p>

 ~~~
만약 태그 속성들은 빼고 그 안에 있는 텍스트만 깔끔하게 가져오고 싶다면 이렇게 써주면 된다 .

~~~phthon
print(soup.p.string)
~~~
None

이제 h1 태그도 한 번 출력해보자.

~~~phthon
print(soup.h1)
~~~



~~~html
<h1 class="svc_name">
	<a class="ci_logo" href="http://www.naver.com/" id="lnb_gonaver" onclick="clickcr(this, 'LNB.naver', '', '', event);" title="naver로 바로가기">
		<img alt="NAVER" height="13" 
src="https://ssl.pstatic.net/static/movie/2013/07/logo_ci.png" width="62"/>
	</a>
	<a class="svc_logo" href="/" onclick="clickcr(this, 'LNB.movie', '', '', event);" title="영화서비스홈으로 바로가기">
		<img alt="영화" height="19" src="https://ssl.pstatic.net/static/movie/2012/06/logo_svc.png" width="34"/>
	</a>
</h1>

~~~

그리고 태그는 보통 트리구조로 위계가 있기 때문에 하위 항목을 모두 뽑아오고 싶다면 `.children`을 사용하면 된다. 예를 들어 ul 태그 안에 리스트가 있다면 이렇게.
~~~py
for child in soup.ul.children:

print(child)
~~~

~~~html
<a class="ci_logo" href="http://www.naver.com/" id="lnb_gonaver" onclick="clickcr(this, 'LNB.naver', '', '', event);" title="naver로 바로가기">
	<img alt="NAVER" height="13" src="https://ssl.pstatic.net/static/movie/2013/07/logo_ci.png" width="62"/>
</a>


<a class="svc_logo" href="/" onclick="clickcr(this, 'LNB.movie', '', '', event);" title="영화서비스홈으로 바로가기">
	<img alt="영화" height="19" src="https://ssl.pstatic.net/static/movie/2012/06/logo_svc.png" width="34"/>
</a>

~~~
당연히 지정된 태그의 상위 항목을 가져올 수도 있다. 이건  `.parents`를 사용한다. 이건 ul 상위에 있는 body 태그를 출력한 후, 전체 html 까지 추가로 출력한다. 계속 상위로 타고 올라가는 거라 생각하면 된다
~~~ py
for parent in soup.ul.parents:

print(parent)
~~~
이번엔 div 태그 하위에 있는 요소들을 하나씩 출력하려면 이렇게 하면 된다.


~~~py
for d in soup.div.children:

print(d)
~~~

##  3. 웹사이트 구조 분석하기 (개발자 도구) ###


위와 같이 태그만으로 텍스트를 뽑아서 웹사이트 구조를 확인하는 건 너무 피곤한 일이다. 그래서 그걸 간편하게 도와줄 개발자 도구를 사용하는 게 좋다. 거의 모든 웹 브라우저는 개발자 도구를 사용할 수 있도록 되어 있다.


크롬에서는 그냥  **F12** 키를 누르면 바로 창이 뜬다. 내가 마우스를 어디에 갖다대는지에 따라 HTML 문서에서 해당하는 부분을 보여준다. 그러니 우리는 이걸 통해 웹사이트에 포함된 태그들을 확인하면 된다.


## 4.  `Select`을 통해 원하는 부분 가져오기


지금까지 태그를 확인하는 감을 익혔다면 이번엔 원하는 부분을 모두 가져올 수 있는  `Select`을 익힐 차례다.


예를 들어 h2 태그를 모두 찾고 싶다고 하면 이렇게.

~~~py
print(soup.select("h2"))
~~~

그러면 결과 값을 리스트로 돌려준다. 이런 식으로
[]
-select 와 find는 기본적으로 같은 역할은 한다  

-find의 경우 .find( 태그이름, class_="클래스이름") 과 같이 지정해줘야 하지만  

-select의 경우 .select(태그이름.클래스이름)과 같이 띄어쓰기 만으로 설정이 가능하다.  

예시

~~~py
from bs4 import
BeautifulSoup import

requests url = "---"
html = requests.get(url)
soup = BeautifulSoup(html.content, 'html.parser')

#find의 경우 title = soup.find('div', class_='gbest-cate')
#select의 경우 title = soup.select_one('div.gbest-cate')  
~~~


-select의 장점은 css 셀렉터를 따라가기 편하다는 것이다.  

-find의 경우 깊숙히 숨어있는 태그를 따기 위해서는 여러 번의 find 메소드를 사용해야하는 반면  

-select의 경우 .select_one('div.classname > div > span) 처럼 더 직관적인 형태로 태그를 쫓을 수 있다.  

~~~py
#find
link = soup.find('div', id='answers-header').div.div.div.a
link['href']

#select
link = soup.select_one('div[id=answers-header] > div > div > div > a')
link['href']
~~~


~~~py
#find
link = soup.find('div', id='answers-header').div.div.div.a
link['href']
#select
link = soup.select_one('div[id=answers-header] > div > div > div > a')
link['href']  
~~~


-나는 select 쪽이 더 아름다운데 여러분은 어떠신가

구글링 결과 select쪽이 timeout도 더 빠르다고 한다  
[https://stackoverflow.com/questions/38028384/beautifulsoup-is-there-a-difference-between-find-and-select-python-3-x](https://stackoverflow.com/questions/38028384/beautifulsoup-is-there-a-difference-between-find-and-select-python-3-x)
## 5. CSS 선택자를 통해 원하는 부분 가져오기

지금까진 html 태그를 중심으로 원하는 부분을 살펴봤지만 이게 생각보다 어려울 수 있다. 해보면 안다. (나만 어렵나?)


그래서 `select()`를 통해 CSS 선택자를 활용하는 방법이 더 수월할 수도 있다.

#### class 앞에는 .(점)을 찍어주기
